{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras import backend as Kb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "print(K.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE\n",
    "---\n",
    "\n",
    "This notebook contains a `Keras / tensorflow` implementation of the `VQ-VAE` model, which was introduced in [*Neural Discrete Representation Learning* (van den Oord et al, NeurIPS 2017)](https://arxiv.org/abs/1711.00937). This is a generative model based on Variational Auto Encoders (`VAE`) which aims to make the latent space discrete using Vector Quantization (`VQ`) techniques. This implementation trains a VQ-VAE based on simple convolutional blocks (no auto-regressive decoder), and a `PixelCNN` categorical prior as described in the paper. The current code was tested on MNIST.\n",
    "\n",
    "**Pros (+):**\n",
    "  * Simple method and training objective\n",
    "  * \"Proper\" Discrete latent space. This is a promising property to model data that is inherently discrete, e.g. text.\n",
    "  \n",
    "\n",
    "**Cons (-):**\n",
    "  * Loses the \"easy latent sampling\" property from VAEs. Two-stage training required to learn a fitting categorical prior. \n",
    "  * The training objective does not correspond to a bound on the log-likelihood anymore.\n",
    "  \n",
    "**Possible improvements, as described in the paper:**\n",
    "  * Learn prior *conditioned on the classes* to even further improve sample generation\n",
    "  * Use a PixelCNN decoder\n",
    "\n",
    "\n",
    "# Dataset and Hyperparameters\n",
    "First we load the dataset (here MNIST). Every image has size 28x28 pixels, 1 color channel (grayscale) and we normalize the color values so that inputs lie in [0., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: (60000, 28, 28, 1)\n",
      "Train labels: (60000,)\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    with np.load(path) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "        return (x_train[..., None] / 255., y_train), (x_test[..., None] / 255., y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data('input/mnist.npz')\n",
    "print(\"Train images:\", x_train.shape)\n",
    "print(\"Train labels:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_LATENT_K = 10                 # Number of codebook entries\n",
    "NUM_LATENT_D = 64                 # Dimension of each codebook entries\n",
    "BETA = 1.0                        # Weight for the commitment loss\n",
    "\n",
    "INPUT_SHAPE = x_train.shape[1:]\n",
    "SIZE = None                       # Spatial size of latent embedding\n",
    "                                  # will be set dynamically in `build_vqvae\n",
    "\n",
    "VQVAE_BATCH_SIZE = 128            # Batch size for training the VQVAE\n",
    "VQVAE_NUM_EPOCHS = 20             # Number of epochs\n",
    "VQVAE_LEARNING_RATE = 3e-4        # Learning rate\n",
    "VQVAE_LAYERS = [16, 32]           # Number of filters for each layer in the encoder\n",
    "\n",
    "PIXELCNN_BATCH_SIZE = 128         # Batch size for training the PixelCNN prior\n",
    "PIXELCNN_NUM_EPOCHS = 10          # Number of epochs\n",
    "PIXELCNN_LEARNING_RATE = 3e-4     # Learning rate\n",
    "PIXELCNN_NUM_BLOCKS = 12          # Number of Gated PixelCNN blocks in the architecture\n",
    "PIXELCNN_NUM_FEATURE_MAPS = 32    # Width of each PixelCNN block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the generative model\n",
    "\n",
    "The first step is to build the main VQ-VAE model. It consists of a standard encoder-decoder architecture with convolutional blocks. The main novelty lies in the intermediate **Vector Quantizer** layer (`VQ`) that takes care of building a **discrete** latent space.\n",
    "\n",
    "More specifically, the encoder, `f` is a fully-convolutional neural network that maps input images to latent codes of size $(w, h, d)$, where $d$ is the dimension of the latent space, and $w \\times h$ the size of the final feature map. The output of the encoder is then mapped to the closest entry in a discrete **codebook** of $K$ latent codes, $\\mathcal E = \\{e_0 \\dots e_{K-1} \\}$ where $\\forall i, e_i \\in \\mathbb{R}^d$.\n",
    "\n",
    "\\begin{align}\n",
    "&\\textbf{input }x \\tag{W x H x C}\\\\\n",
    "z_e &= f(x) \\tag{w x h x d}\\\\\n",
    "z_q^{i, j} &= \\arg\\min_{e \\in \\mathcal E} \\| z_e^{i, j} - e \\|^2\n",
    "\\end{align}\n",
    "\n",
    "The Vector Quantization process is implemented as the following `Keras` Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(K.layers.Layer):  \n",
    "    def __init__(self, k, **kwargs):\n",
    "        super(VectorQuantizer, self).__init__(**kwargs)\n",
    "        self.k = k\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d = int(input_shape[-1])\n",
    "        rand_init = K.initializers.VarianceScaling(distribution=\"uniform\")\n",
    "        self.codebook = self.add_weight(shape=(self.k, self.d), initializer=rand_init, trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Map z_e of shape (b, w,, h, d) to indices in the codebook\n",
    "        lookup_ = tf.reshape(self.codebook, shape=(1, 1, 1, self.k, self.d))\n",
    "        z_e = tf.expand_dims(inputs, -2)\n",
    "        dist = tf.norm(z_e - lookup_, axis=-1)\n",
    "        k_index = tf.argmin(dist, axis=-1)\n",
    "        return k_index\n",
    "    \n",
    "    def sample(self, k_index):\n",
    "        # Map indices array of shape (b, w, h) to actual codebook z_q\n",
    "        lookup_ = tf.reshape(self.codebook, shape=(1, 1, 1, self.k, self.d))\n",
    "        k_index_one_hot = tf.one_hot(k_index, self.k)\n",
    "        z_q = lookup_ * k_index_one_hot[..., None]\n",
    "        z_q = tf.reduce_sum(z_q, axis=-2)\n",
    "        return z_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder, $g$, then takes the quantized codes $z_q$ as inputs and generates the output image. Here we consider a simple architecture with transposed convolution blocks, mirroring the encoder architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_pass(inputs, d, num_layers=[16, 32]):\n",
    "    x = inputs\n",
    "    for i, filters in enumerate(num_layers):\n",
    "        x = K.layers.Conv2D(filters=filters, kernel_size=3, padding='SAME', activation='relu', \n",
    "                            strides=(2, 2), name=\"conv{}\".format(i + 1))(x)\n",
    "    z_e = K.layers.Conv2D(filters=d, kernel_size=3, padding='SAME', activation=None,\n",
    "                          strides=(1, 1), name='z_e')(x)\n",
    "    return z_e\n",
    "\n",
    "def decoder_pass(inputs, num_layers=[32, 16]):\n",
    "    y = inputs\n",
    "    for i, filters in enumerate(num_layers):\n",
    "        y = K.layers.Conv2DTranspose(filters=filters, kernel_size=4, strides=(2, 2), padding=\"SAME\", \n",
    "                                     activation='relu', name=\"convT{}\".format(i + 1))(y)\n",
    "    decoded = K.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=(1, 1), \n",
    "                                       padding=\"SAME\", activation='sigmoid', name='output')(y)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once these three building blocks are done, we can build the full `VQ-VAE`. One subtility is how we can estimate gradient through the Vector Quantizer: In fact, the transition from $z_e$ to $z_q$ does not allow to backpropagate gradient due to the argmin function. Instead, the authors propose to use a *straight-through estimator*, that directly copies the gradient received by $z_q$ to $z_e$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (sample_from_codebook), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'vector_quantizer/Variable:0' shape=(10, 64) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "Model: \"vq-vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 14, 14, 16)   160         encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 7, 7, 32)     4640        conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "z_e (Conv2D)                    (None, 7, 7, 64)     18496       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "vector_quantizer (VectorQuantiz (None, 7, 7)         640         z_e[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "sample_from_codebook (Lambda)   (None, 7, 7, 64)     0           vector_quantizer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "straight_through_estimator (Lam (None, 7, 7, 64)     0           sample_from_codebook[0][0]       \n",
      "                                                                 z_e[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tf.stack (TFOpLambda)           (None, 7, 7, 64, 2)  0           z_e[0][0]                        \n",
      "                                                                 sample_from_codebook[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, 28, 28, 1)    41153       straight_through_estimator[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "latent_codes (Lambda)           (None, 7, 7, 64, 2)  0           tf.stack[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 65,089\n",
      "Trainable params: 65,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_vqvae(k, d, input_shape=(28, 28, 1), num_layers=[16, 32]):\n",
    "    global SIZE\n",
    "    ## Encoder\n",
    "    encoder_inputs = K.layers.Input(shape=input_shape, name='encoder_inputs')\n",
    "    z_e = encoder_pass(encoder_inputs, d, num_layers=num_layers)\n",
    "    SIZE = int(z_e.get_shape()[1])\n",
    "\n",
    "    ## Vector Quantization\n",
    "    vector_quantizer = VectorQuantizer(k, name=\"vector_quantizer\")\n",
    "    codebook_indices = vector_quantizer(z_e)\n",
    "    encoder = K.Model(inputs=encoder_inputs, outputs=codebook_indices, name='encoder')\n",
    "\n",
    "    ## Decoder\n",
    "    decoder_inputs = K.layers.Input(shape=(SIZE, SIZE, d), name='decoder_inputs')\n",
    "    decoded = decoder_pass(decoder_inputs, num_layers=num_layers[::-1])\n",
    "    decoder = K.Model(inputs=decoder_inputs, outputs=decoded, name='decoder')\n",
    "    \n",
    "    ## VQVAE Model (training)\n",
    "    sampling_layer = K.layers.Lambda(lambda x: vector_quantizer.sample(x), name=\"sample_from_codebook\")\n",
    "    z_q = sampling_layer(codebook_indices)\n",
    "    codes = tf.stack([z_e, z_q], axis=-1)\n",
    "    codes = K.layers.Lambda(lambda x: x, name='latent_codes')(codes)\n",
    "    straight_through = K.layers.Lambda(lambda x : x[1] + tf.stop_gradient(x[0] - x[1]), name=\"straight_through_estimator\")\n",
    "    straight_through_zq = straight_through([z_q, z_e])\n",
    "    reconstructed = decoder(straight_through_zq)\n",
    "    vq_vae = K.Model(inputs=encoder_inputs, outputs=[reconstructed, codes], name='vq-vae')\n",
    "    \n",
    "    ## VQVAE model (inference)\n",
    "    codebook_indices = K.layers.Input(shape=(SIZE, SIZE), name='discrete_codes', dtype=tf.int32)\n",
    "    z_q = sampling_layer(codebook_indices)\n",
    "    generated = decoder(z_q)\n",
    "    vq_vae_sampler = K.Model(inputs=codebook_indices, outputs=generated, name='vq-vae-sampler')\n",
    "    \n",
    "    ## Transition from codebook indices to model (for training the prior later)\n",
    "    indices = K.layers.Input(shape=(SIZE, SIZE), name='codes_sampler_inputs', dtype='int32')\n",
    "    z_q = sampling_layer(indices)\n",
    "    codes_sampler = K.Model(inputs=indices, outputs=z_q, name=\"codes_sampler\")\n",
    "    \n",
    "    ## Getter to easily access the codebook for vizualisation\n",
    "    indices = K.layers.Input(shape=(), dtype='int32')\n",
    "    vector_model = K.Model(inputs=indices, outputs=vector_quantizer.sample(indices[:, None, None]), name='get_codebook')\n",
    "    def get_vq_vae_codebook():\n",
    "        codebook = vector_model.predict(np.arange(k))\n",
    "        codebook = np.reshape(codebook, (k, d))\n",
    "        return codebook\n",
    "    \n",
    "    return vq_vae, vq_vae_sampler, encoder, decoder, codes_sampler, get_vq_vae_codebook\n",
    "\n",
    "vq_vae, vq_vae_sampler, encoder, decoder, codes_sampler, get_vq_vae_codebook = build_vqvae(\n",
    "    NUM_LATENT_K, NUM_LATENT_D, input_shape=INPUT_SHAPE, num_layers=VQVAE_LAYERS)\n",
    "vq_vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the VQ-VAE\n",
    "\n",
    "All is left now is to train the model: The training objective contains the **reconstruction loss** (here, we use mean squared error), the KL divergence term on the latent codebook (ignored because it is constant as we assume a uniform prior during training), and two **vector quantization losses** which guarantee that **(i)** the outputs of the encoder stay close to the codebook entries they are matched to and **(ii)** that the codebook does not grow too much relatively to the space of the encoder outputs.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal L_{\\text{VQ-VAE}}(x) = - \\mathbb{E}_{z \\sim f(x)}{p(x | z)} + \\| z_e - \\bar{z_q}\\|^2 + \\|\\bar{z_e} - z_q\\|^2\n",
    "\\end{align}\n",
    "\n",
    "where $\\bar{\\cdot}$ denotes the stop gradient operation: i.e., during forward pass, this corresponds to the identity, but during backpropagation no gradients are flowing through this operation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(ground_truth, predictions):\n",
    "    mse_loss = tf.reduce_mean((ground_truth - predictions)**2, name=\"mse_loss\")\n",
    "    return mse_loss\n",
    "\n",
    "def latent_loss(dummy_ground_truth, outputs):\n",
    "    global BETA\n",
    "    del dummy_ground_truth\n",
    "    z_e, z_q = tf.split(outputs, 2, axis=-1)\n",
    "    vq_loss = tf.reduce_mean((tf.stop_gradient(z_e) - z_q)**2)\n",
    "    commit_loss = tf.reduce_mean((z_e - tf.stop_gradient(z_q))**2)\n",
    "    latent_loss = tf.identity(vq_loss + BETA * commit_loss, name=\"latent_loss\")\n",
    "    return latent_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Additionally to the losses, we're also going to monitor the $L2$ norms of the encoded vectors (both output by the encoder and after quantization), represented as `Keras` metrics here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zq_norm(y_true, y_pred):\n",
    "    del y_true\n",
    "    _, z_q = tf.split(y_pred, 2, axis=-1)\n",
    "    return tf.reduce_mean(tf.norm(z_q, axis=-1))\n",
    "\n",
    "def ze_norm(y_true, y_pred):\n",
    "    del y_true\n",
    "    z_e, _ = tf.split(y_pred, 2, axis=-1)\n",
    "    return tf.reduce_mean(tf.norm(z_e, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\", \"<class 'NoneType'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kwamejob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1048\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1051\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kwamejob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kwamejob\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    959\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[1;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    962\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m         \"input: {}, {}\".format(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'numpy.ndarray'>\", \"<class 'NoneType'>\"})"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Compile and train\n",
    "vq_vae.compile(loss=[mse_loss, latent_loss], metrics={\"latent_codes\": [zq_norm, ze_norm]}, optimizer= K.optimizers.Adam(VQVAE_LEARNING_RATE))\n",
    "history = vq_vae.fit(x_train, [x_train, None], epochs=VQVAE_NUM_EPOCHS, batch_size=VQVAE_BATCH_SIZE, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "num_epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_epochs, history.history[\"loss\"], label=\"total_loss\")\n",
    "plt.plot(num_epochs, history.history[\"decoder_loss\"], label=\"recons_loss\")\n",
    "plt.plot(num_epochs, history.history[\"latent_codes_loss\"], label=\"vq_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss during training\")\n",
    "plt.legend()\n",
    "plt.title(\"Losses\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_epochs, history.history[\"latent_codes_ze_norm\"], label=\"ze_norm\")\n",
    "plt.plot(num_epochs, history.history[\"latent_codes_zq_norm\"], label=\"zq_norm\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Norm\")\n",
    "plt.title(\"Codes norms during training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is done, we can also visualize some of the results, such as reconstructions on the test set and the learned codebook entries (projected to 2D with `TSNE`). In particular, we observe that reconstructions are close to perfect, which indicates the model is able to learn a meaningful codebook, as well as how to map images to this discrete space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# original versus reconstructions\n",
    "def recons(model, images, n_row, n_col, random=True):\n",
    "    n = n_row * n_col\n",
    "    if random:\n",
    "        x = np.random.choice(images.shape[0], size=n, replace=False)\n",
    "        x = images[x]\n",
    "    else:\n",
    "        x = images[:n]\n",
    "    recons, _ = model.predict(x)\n",
    "    dists = np.mean((recons - x)**2, axis=(1, 2, 3))\n",
    "    plt.figure(figsize=(15, 9))\n",
    "    for i in range(n):\n",
    "        plt.subplot(n_row, 2 * n_col, 2 * i + 1)\n",
    "        plt.imshow(x[i, :, :, 0], cmap='gray')\n",
    "        plt.title(\"original\", fontsize=7)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(n_row, 2 * n_col, 2 * i + 2)\n",
    "        plt.imshow(recons[i, :, :, 0], cmap='gray')\n",
    "        plt.title(\"L2: {:.3f}\".format(dists[i]), fontsize=7)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Originals versus reconstructions\")\n",
    "recons(vq_vae, x_test, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "codebook = get_vq_vae_codebook()\n",
    "codebook = TSNE(2).fit_transform(codebook)\n",
    "plt.title(\"Codebook (k={}, d={}) (TSNE projected)\".format(NUM_LATENT_K, NUM_LATENT_D))\n",
    "plt.scatter(codebook[:, 0], codebook[:, 1])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a prior over the latent space\n",
    "---\n",
    "We have now learned an encoder-decoder architecture and a discrete latent codebook powerful enough to encode and reconstruct our dataset. However, the **uniform prior** assumption during training is not sufficient for generating good samples. In fact, due to the fully-convolutional architecture, each image is encoded with `SIZE` x `SIZE` latent vectors from the codebook (for instance, `SIZE = 7` for our current model).\n",
    "\n",
    "However, the codes for our dataset have no guarantee to lie uniformly on that space, as we assumed during training, but rather have some specific structure that follow a certain non-uniform categorical prior. This can be seen easily by generating images from code feature maps sampled uniformly from the total latent space of size `SIZE` x `SIZE` x $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def generate_from_random(sampler, k, size, n_row, n_col):\n",
    "    n = n_col * n_row\n",
    "    indices = np.random.randint(0, k, size=(n, size, size))\n",
    "    generated = sampler.predict(indices, steps=1)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n):\n",
    "        plt.subplot(n_row,n_col,i+1)\n",
    "        plt.imshow(generated[i,:,:,0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Images generated under a uniform prior (training assumption)\")\n",
    "generate_from_random(vq_vae_sampler, NUM_LATENT_K, SIZE, 10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem and sample likely codes from the latent space, the authors propose to learn a **powerful categorical prior** over the latent codes from the training images using a `PixelCNN`. PixelCNN is a fully probabilistic autoregressive generative model that generates images (or here, feature maps) pixel by pixel, conditioned on the previously generated pixels. The main drawback of such models is that the sampling process is rather slow. \n",
    "However, since here we are only generating small `SIZE` x `SIZE` maps, the overhead is not too bad.\n",
    "\n",
    "Here we consider the architecture proposed in [*Conditional Image Generation with PixelCNN Decoders* (van den Oord et al, NeurIPS 2017)](https://arxiv.org/abs/1606.05328) which uses **gated** and **masked** convolutions to model the fact that pixels only depend from the previously generated context. We implement the base building block of the architecture as the following `Keras` pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# https://github.com/anantzoid/Conditional-PixelCNN-decoder/blob/master/layers.py\n",
    "# https://github.com/ritheshkumar95/pytorch-vqvae\n",
    "\n",
    "def gate(inputs):\n",
    "    \"\"\"Gated activations\"\"\"\n",
    "    x, y = tf.split(inputs, 2, axis=-1)\n",
    "    return Kb.tanh(x) * Kb.sigmoid(y)\n",
    "\n",
    "\n",
    "class MaskedConv2D(K.layers.Layer):\n",
    "    \"\"\"Masked convolution\"\"\"\n",
    "    def __init__(self, kernel_size, out_dim, direction, mode, **kwargs):\n",
    "        self.direction = direction     # Horizontal or vertical\n",
    "        self.mode = mode               # Mask type \"a\" or \"b\"\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_dim = out_dim\n",
    "        super(MaskedConv2D, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):   \n",
    "        filter_mid_y = self.kernel_size[0] // 2\n",
    "        filter_mid_x = self.kernel_size[1] // 2        \n",
    "        in_dim = int(input_shape[-1])\n",
    "        w_shape = [self.kernel_size[0], self.kernel_size[1], in_dim, self.out_dim]\n",
    "        mask_filter = np.ones(w_shape, dtype=np.float32)\n",
    "        # Build the mask\n",
    "        if self.direction == \"h\":\n",
    "            mask_filter[filter_mid_y + 1:, :, :, :] = 0.\n",
    "            mask_filter[filter_mid_y, filter_mid_x + 1:, :, :] = 0.\n",
    "        elif self.direction == \"v\":\n",
    "            if self.mode == 'a':\n",
    "                mask_filter[filter_mid_y:, :, :, :] = 0.\n",
    "            elif self.mode == 'b':\n",
    "                mask_filter[filter_mid_y+1:, :, :, :] = 0.0\n",
    "        if self.mode == 'a':\n",
    "            mask_filter[filter_mid_y, filter_mid_x, :, :] = 0.0\n",
    "        # Create convolution layer parameters with masked kernel\n",
    "        self.W = mask_filter * self.add_weight(\"W_{}\".format(self.direction), w_shape, trainable=True)\n",
    "        self.b = self.add_weight(\"v_b\", [self.out_dim,], trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return K.backend.conv2d(inputs, self.W, strides=(1, 1)) + self.b\n",
    "\n",
    "    \n",
    "def gated_masked_conv2d(v_stack_in, h_stack_in, out_dim, kernel, mask='b', residual=True, i=0):\n",
    "    \"\"\"Basic Gated-PixelCNN block. \n",
    "       This is an improvement over PixelRNN to avoid \"blind spots\", i.e. pixels missingt from the\n",
    "       field of view. It works by having two parallel stacks, for the vertical and horizontal direction, \n",
    "       each being masked  to only see the appropriate context pixels.\n",
    "    \"\"\"\n",
    "    kernel_size = (kernel // 2 + 1, kernel)\n",
    "    padding = (kernel // 2, kernel // 2)\n",
    "        \n",
    "    v_stack = K.layers.ZeroPadding2D(padding=padding, name=\"v_pad_{}\".format(i))(v_stack_in)\n",
    "    v_stack = MaskedConv2D(kernel_size, out_dim * 2, \"v\", mask, name=\"v_masked_conv_{}\".format(i))(v_stack)\n",
    "    v_stack = v_stack[:, :int(v_stack_in.get_shape()[-3]), :, :]\n",
    "    v_stack_out = K.layers.Lambda(lambda inputs: gate(inputs), name=\"v_gate_{}\".format(i))(v_stack)\n",
    "    \n",
    "    kernel_size = (1, kernel // 2 + 1)\n",
    "    padding = (0, kernel // 2)\n",
    "    h_stack = K.layers.ZeroPadding2D(padding=padding, name=\"h_pad_{}\".format(i))(h_stack_in)\n",
    "    h_stack = MaskedConv2D(kernel_size, out_dim * 2, \"h\", mask, name=\"h_masked_conv_{}\".format(i))(h_stack)\n",
    "    h_stack = h_stack[:, :, :int(h_stack_in.get_shape()[-2]), :]\n",
    "    h_stack_1 = K.layers.Conv2D(filters=out_dim * 2, kernel_size=1, strides=(1, 1), name=\"v_to_h_{}\".format(i))(v_stack)\n",
    "    h_stack_out = K.layers.Lambda(lambda inputs: gate(inputs), name=\"h_gate_{}\".format(i))(h_stack + h_stack_1)\n",
    "    \n",
    "    h_stack_out =  K.layers.Conv2D(filters=out_dim, kernel_size=1, strides=(1, 1), name=\"res_conv_{}\".format(i))(h_stack_out)\n",
    "    if residual:\n",
    "        h_stack_out += h_stack_in\n",
    "    return v_stack_out, h_stack_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior training data\n",
    "\n",
    "In order to train the prior, we're going to encode every training image to obtain their discrete representation as indices in the latent codebook. This is also a good opportunity to **visualize the discrete representations learned by the encoder**. Here we can notice some interesting features, such as: all the black/background pixels are mapped to the same codeword, and the same for dense/white pixels (e.g., the ones at the center of a number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to train the Prior\n",
    "z_train = encoder.predict(x_train)\n",
    "z_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# original versus reconstructions\n",
    "def viz_codes(encoder, images, n_row, n_col, random=True):\n",
    "    n = n_row * n_col\n",
    "    if random:\n",
    "        x = np.random.choice(images.shape[0], size=n, replace=False)\n",
    "        x = images[x]\n",
    "    else:\n",
    "        x = images[:n]\n",
    "    codes = encoder.predict(x)\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    for i in range(n):\n",
    "        plt.subplot(n_row, 2 * n_col, 2*i+1)\n",
    "        plt.imshow(x[i,:,:,0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(n_row, 2 * n_col, 2*i+2)\n",
    "        plt.pcolor(codes[i, ::-1, :], cmap='tab10') #flip y-axis origin to match imshow\n",
    "        plt.axis('off')\n",
    "    cbaxes = fig.add_axes([0.92, 0.15, 0.015, 0.7]) \n",
    "    cb = plt.colorbar(cax=cbaxes)  \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "print(\"Images encoded as discrete codes\")\n",
    "viz_codes(encoder, x_test, 8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipped with the Masked gated convolutions and the training set, we can finally build our PixelCNN architecture and train the prior. The full model simply consists in a concatenation of masked and gated convolutions, followed by two fully-connected layers to output the final prediction. Here the training objective is a multi-class classification one, as the prior should output a map of codebook **indices**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pixelcnn(codes_sampler, k, size, num_layers, num_feature_maps=32):\n",
    "    pixelcnn_prior_inputs = K.layers.Input(shape=(size, size), name='pixelcnn_prior_inputs', dtype=tf.int32)\n",
    "    z_q = codes_sampler(pixelcnn_prior_inputs) # maps indices to the actual codebook\n",
    "    \n",
    "    v_stack_in, h_stack_in = z_q, z_q\n",
    "    for i in range(num_layers):\n",
    "        mask = 'b' if i > 0 else 'a'\n",
    "        kernel_size = 3 if i > 0 else 7\n",
    "        residual = True if i > 0 else False\n",
    "        v_stack_in, h_stack_in = gated_masked_conv2d(v_stack_in, h_stack_in, num_feature_maps,\n",
    "                                                     kernel=kernel_size, residual=residual, i=i + 1)\n",
    "\n",
    "    fc1 = K.layers.Conv2D(filters=num_feature_maps, kernel_size=1, name=\"fc1\")(h_stack_in)\n",
    "    fc2 = K.layers.Conv2D(filters=k, kernel_size=1, name=\"fc2\")(fc1) \n",
    "    # outputs logits for probabilities of codebook indices for each cell\n",
    "\n",
    "    pixelcnn_prior = K.Model(inputs=pixelcnn_prior_inputs, outputs=fc2, name='pixelcnn-prior')\n",
    "\n",
    "    # Distribution to sample from the pixelcnn\n",
    "    dist = tf.distributions.Categorical(logits=fc2)\n",
    "    sampled = dist.sample()\n",
    "    prior_sampler = K.Model(inputs=pixelcnn_prior_inputs, outputs=sampled, name='pixelcnn-prior-sampler')\n",
    "    return pixelcnn_prior, prior_sampler\n",
    "\n",
    "pixelcnn_prior, prior_sampler = build_pixelcnn(codes_sampler, NUM_LATENT_K, SIZE, \n",
    "                                               PIXELCNN_NUM_BLOCKS, PIXELCNN_NUM_FEATURE_MAPS)\n",
    "#pixelcnn_prior.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the PixelCNN and monitor prediction accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    size = int(y_pred.get_shape()[-2])\n",
    "    k = int(y_pred.get_shape()[-1])\n",
    "    y_true = tf.reshape(y_true, (-1, size * size))\n",
    "    y_pred = tf.reshape(y_pred, (-1, size * size, k))\n",
    "    return Kb.cast(Kb.equal(y_true, Kb.cast(Kb.argmax(y_pred, axis=-1), Kb.floatx())), Kb.floatx())\n",
    "\n",
    "pixelcnn_prior.compile(loss=K.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[accuracy],\n",
    "                       optimizer=K.optimizers.Adam(PIXELCNN_LEARNING_RATE))\n",
    "\n",
    "prior_history = pixelcnn_prior.fit(z_train, z_train, epochs=PIXELCNN_NUM_EPOCHS, \n",
    "                                   batch_size=PIXELCNN_BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "num_epochs = range(1, len(prior_history.history[\"loss\"]) + 1)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(num_epochs, prior_history.history[\"loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss during training\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(num_epochs, prior_history.history[\"accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy during training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can check the model ability to **reconstruct** discrete latent codes obtained from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def recons_prior(model, z, n_row, n_col, random=True):\n",
    "    n = n_row * n_col\n",
    "    if random:\n",
    "        x = np.random.choice(z.shape[0], size=n, replace=False)\n",
    "        x = z[x]\n",
    "    else:\n",
    "        x = z[:n]\n",
    "    recons = model.predict(x)\n",
    "    recons = np.argmax(recons, axis=-1)\n",
    "    accs = np.mean(np.equal(recons, x), axis=(1, 2))\n",
    "    plt.figure(figsize=(15, 9))\n",
    "    for i in range(n):\n",
    "        plt.subplot(n_row, 2 * n_col, 2 * i + 1)\n",
    "        plt.imshow(x[i], cmap='tab10')\n",
    "        plt.title(\"original\", fontsize=8)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(n_row, 2 * n_col, 2 * i + 2)\n",
    "        plt.imshow(recons[i], cmap='tab10')\n",
    "        plt.title(\"acc: {:.3f}\".format(accs[i]), fontsize=8)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Originals versus reconstructions\")\n",
    "z_test = encoder.predict(x_test)\n",
    "recons_prior(pixelcnn_prior, z_test, 8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More importantly, let's have a look at images generated by sampling from the prior. As expected, they look much better than sampling from a uniform distribution, which means that **(i)** discrete codes for our image distribution lie in a specific subset of the latent space and **(ii)** the PixelCNN was able to properly model a prior probability distribution on that space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def sample_from_prior(prior, shape):\n",
    "    \"\"\"sample from the PixelCNN prior, pixel by pixel\"\"\"\n",
    "    global prior_sampler\n",
    "    X = np.zeros(shape, dtype=np.int32)\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(X.shape[2]):\n",
    "            sampled = prior_sampler.predict(X)\n",
    "            X[:, i, j] = sampled[:, i, j]\n",
    "    return X\n",
    "            \n",
    "    \n",
    "def generate(prior, codes_sampler, size, n_row, n_col):\n",
    "    \"\"\"Generate random images by sampling codes from the given prior\"\"\"\n",
    "    n = n_col * n_row\n",
    "    indices = sample_from_prior(prior, (n, size, size))\n",
    "    zq = codes_sampler(indices)\n",
    "    generated = decoder.predict(zq, steps=1)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(n):\n",
    "        plt.subplot(n_row, 2 * n_col, 2 * i + 1)\n",
    "        plt.imshow(indices[i], cmap='tab10')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(n_row, 2 * n_col, 2 * i + 2)\n",
    "        plt.imshow(generated[i,:,:,0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Random codes sampled from the prior and correspopnding decoded images\")\n",
    "generate(prior_sampler, codes_sampler, SIZE, 10, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
